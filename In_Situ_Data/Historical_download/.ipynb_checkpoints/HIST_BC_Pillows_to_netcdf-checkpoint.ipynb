{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "#mpld3.enable_notebook()\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import xarray as xr\n",
    "from astropy.io import ascii\n",
    "import pytz\n",
    "# OS interaction\n",
    "import sys\n",
    "import os\n",
    "import glob\n",
    "import wget\n",
    "import seaborn as sns\n",
    "sns.set_context(\"talk\",font_scale=1.5)\n",
    "sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Paths to user files\n",
    "data_dir = os.path.normpath(r'F:\\Work\\e\\Data\\Obs\\Canada_Project_Sites\\CSAS_data') # Where to store data on local computer\n",
    "git_dir  = os.path.normpath(r'C:\\Users\\new356\\Google Drive\\Python\\SnowCast\\In_Situ_Data') # This repo\n",
    "\n",
    "# ascii_dir  = os.path.normpath(r'F:\\Work\\e\\Data\\Obs\\Canada_Project_Sites\\EC\\AlbertaMetData\\ascii')\n",
    "# netcdf_dir = os.path.normpath(r'F:\\Work\\e\\Data\\Obs\\Canada_Project_Sites\\EC\\AlbertaMetData\\netcdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data network\n",
    "network = 'BC_HIST'\n",
    "\n",
    "# Location to download historical AB station data\n",
    "download_dir = os.path.join(data_dir,network,'current')\n",
    "# Make if does not exist\n",
    "if not os.path.exists(download_dir):\n",
    "    os.makedirs(download_dir)\n",
    "    \n",
    "# Netcdf file to save to\n",
    "netcdf_dir   = os.path.join(data_dir,network,'netcdf')\n",
    "# Make if does not exist\n",
    "if not os.path.exists(netcdf_dir):\n",
    "    os.makedirs(netcdf_dir)\n",
    "netcdf_file_out =  os.path.join(netcdf_dir,'BC_HIST.nc')\n",
    "\n",
    "# Metadata for AB pillows \n",
    "meta_file         = 'BC_Station_Metadata.csv'\n",
    "meta_file_path    = os.path.join(git_dir,'metadata',meta_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Download Historical BC data (Updated at end of water year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "os.chdir(download_dir)\n",
    "# File format\n",
    "# \n",
    "# File names change only by last water year, so figure this out based on current date\n",
    "l_WY = '2016'\n",
    "BC_files = ['Raw Snow Water Equivalent (mm) October 1 2011 to September 30 '+l_WY+'.csv', \n",
    "           'Raw Cumulative Precipitation (mm) October 1 2011 to September 30 '+l_WY+'.csv',\n",
    "            'Raw Snow Depth (cm) October 1 2011 to September 30 '+l_WY+'.csv',\n",
    "           'Raw Air Temperature (Degrees Celsius) October 1 2011 to September 30 '+l_WY+'.csv']\n",
    "Var_names = ['SWE','Precipitation','Snowdepth','AirTemperature']\n",
    "Var_units = ['mm','mm','cm','C']\n",
    "c_network = 'bcRiverForecastCenter'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Remove previous files\n",
    "for cfile in BC_files:\n",
    "    try:\n",
    "        os.remove(cfile)\n",
    "    except OSError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100% [....................................................] 10571050 / 10571050"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Raw Snow Water Equivalent (mm) October 1 2011 to September 30 2016.csv',\n",
       " 'Raw Cumulative Precipitation (mm) October 1 2011 to September 30 2016.csv',\n",
       " 'Raw Snow Depth (cm) October 1 2011 to September 30 2016.csv',\n",
       " 'Raw Air Temperature (Degrees Celsius) October 1 2011 to September 30 2016.csv']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download newest files\n",
    "url_base         = 'http://bcrfc.env.gov.bc.ca/data/asp/'\n",
    "[wget.download(url_base+cfile) for cfile in BC_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import metadata for each station\n",
    "metadata = pd.read_csv(meta_file_path,index_col=1,delimiter=',',encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Snow Water Equivalent (mm) October 1 2011 to September 30 2016.csv\n",
      "Raw Cumulative Precipitation (mm) October 1 2011 to September 30 2016.csv\n",
      "Raw Snow Depth (cm) October 1 2011 to September 30 2016.csv\n",
      "Raw Air Temperature (Degrees Celsius) October 1 2011 to September 30 2016.csv\n"
     ]
    }
   ],
   "source": [
    "# Import each data variable\n",
    "ds_dict   = {}\n",
    "unit_dict = {}\n",
    "for (i,cf) in enumerate(BC_files):\n",
    "    print(cf)\n",
    "    \n",
    "    # Load in to python\n",
    "    df = pd.read_csv(cf,index_col=0, skipfooter=1, engine='python', parse_dates=True)\n",
    "    var_name_full  = Var_names[i]\n",
    "    var_units = Var_units[i]\n",
    "    df.index.names = ['Time_UTC']\n",
    "    \n",
    "#     # Check for error in PC.csv\n",
    "#     if cf=='PC.csv':\n",
    "#         print('fixing Muskwa-Kechik error')\n",
    "#         df = df.rename(columns = {'4A34P Muskwa-Kechika':'4A34P Dowling Creek'})\n",
    "\n",
    "    # Store as dict\n",
    "    ds_dict[var_name_full] = df\n",
    "    unit_dict[var_name_full] = var_units\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Merge into netcdf\n",
    "ds = xr.Dataset(ds_dict)\n",
    "ds.rename({'dim_1':'staID'},inplace=True) # rename time\n",
    "ds['staID']        = [str(x).split(' ')[0] for x in ds.staID.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## ADD UNITS\n",
    "# Add variable attributes (units), and fix variable names (remove spaces)\n",
    "for cvar in ds.data_vars:\n",
    "    # add units as attributes\n",
    "    ds.get(cvar).attrs['unit']   = unit_dict[cvar]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Add station metadata\n",
    "ds['station_name'] = xr.DataArray(metadata['station'],coords={'staID':metadata.index}, dims='staID')\n",
    "ds['Lat'] = xr.DataArray(metadata['latitude'],coords={'staID':metadata.index}, dims='staID')\n",
    "ds['Lon'] = xr.DataArray(metadata['longitude'],coords={'staID':metadata.index}, dims='staID')\n",
    "ds['Elevation'] = xr.DataArray(metadata['elevation'],coords={'staID':metadata.index}, dims='staID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.Dataset>\n",
       "Dimensions:         (Time_UTC: 44162, staID: 66)\n",
       "Coordinates:\n",
       "  * staID           (staID) object '1A01P' '1A02P' '1A03P' '1A12P' '1A14P' ...\n",
       "  * Time_UTC        (Time_UTC) datetime64[ns] 2011-06-09T23:00:00 2011-06-10 ...\n",
       "    station_name    (staID) object 'Yellowhead Lake' 'McBride' 'Barkerville' ...\n",
       "    Lat             (staID) float64 52.91 53.31 53.06 56.02 54.11 54.3 53.78 ...\n",
       "    Lon             (staID) float64 -118.5 -120.3 -121.5 -126.3 -121.0 ...\n",
       "    Elevation       (staID) float64 1.852e+03 1.58e+03 1.48e+03 1.257e+03 ...\n",
       "Data variables:\n",
       "    Snowdepth       (Time_UTC, staID) float64 nan nan nan nan nan nan nan ...\n",
       "    AirTemperature  (Time_UTC, staID) float64 nan nan nan nan nan nan nan ...\n",
       "    SWE             (Time_UTC, staID) float64 nan nan nan nan nan nan nan ...\n",
       "    Precipitation   (Time_UTC, staID) float64 nan nan nan nan nan nan nan ..."
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Move to coords\n",
    "ds.set_coords(['station_name','Lat','Lon','Elevation'], inplace=True)\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Reindex to have continous time steps\n",
    "Time_UTC_new = np.arange(ds.Time_UTC.values[0], ds.Time_UTC.values[-1], dtype='datetime64[h]')\n",
    "ds_fill = ds.reindex({'Time_UTC':Time_UTC_new})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add Network\n",
    "ds_fill.coords['network'] = xr.DataArray([c_network for x in ds_fill.staID], dims='staID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save as netcdf file\n",
    "ds_fill.to_netcdf(netcdf_file_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
