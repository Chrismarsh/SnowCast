{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "#mpld3.enable_notebook()\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import xarray as xr\n",
    "from astropy.io import ascii\n",
    "import pytz\n",
    "# OS interaction\n",
    "import sys\n",
    "import os\n",
    "import glob\n",
    "import wget\n",
    "import seaborn as sns\n",
    "sns.set_context(\"talk\",font_scale=1.5)\n",
    "sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Paths to user files\n",
    "data_dir = os.path.normpath(r'F:\\Work\\e\\Data\\Obs\\Canada_Project_Sites\\CSAS_data') # Where to store data on local computer\n",
    "git_dir  = os.path.normpath(r'C:\\Users\\new356\\Google Drive\\Python\\SnowCast\\In_Situ_Data') # This repo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create paths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Data network\n",
    "network = 'ABE_AGG_HIST'\n",
    "\n",
    "# Location to download historical AB station data\n",
    "download_dir = os.path.join(data_dir,network,'current')\n",
    "# Make if does not exist\n",
    "if not os.path.exists(download_dir):\n",
    "    os.makedirs(download_dir)\n",
    "    \n",
    "# Netcdf file to save to\n",
    "netcdf_dir   = os.path.join(data_dir,network,'netcdf')\n",
    "# Make if does not exist\n",
    "if not os.path.exists(netcdf_dir):\n",
    "    os.makedirs(netcdf_dir)\n",
    "netcdf_file_out =  os.path.join(netcdf_dir,'ABE_AGG_HIST.nc')\n",
    "\n",
    "# # Metadata for AB pillows \n",
    "meta_file         = 'ABE_AGG_Station_Metadata.csv'\n",
    "meta_file_path    = os.path.join(git_dir,'metadata',meta_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "os.chdir(download_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sta_files = ['HourlyPrecipMatrix.csv',\n",
    "            'HourlyRHMatrix.csv',\n",
    "            'HourlyTaMatrix.csv',\n",
    "            'HourlyUDMatrix.csv',\n",
    "            'HourlyUSMatrix.csv']\n",
    "\n",
    "Var_names = ['Precipitation','RealtiveHumidity','AirTemperature','WindDirection','WindSpeed']\n",
    "Var_units = ['mm','%','C','m/s','degrees'] # not windspeed comes in as km/h, converted below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import each data variable\n",
    "ds_dict   = {}\n",
    "hrd_dict = {}\n",
    "unit_dict = {}\n",
    "for (i,cf) in enumerate(sta_files):\n",
    "    print(cf)\n",
    "    \n",
    "    # Load in to python\n",
    "    df_all = pd.read_csv(cf,index_col=0, engine='python', parse_dates=True, na_values=['---'])\n",
    "    df_hdr_raw = df_all[0:14]\n",
    "    df_dat = df_all[16:].convert_objects(convert_numeric=True) # Force to floats\n",
    "    \n",
    "    # Clean up messy header info\n",
    "    df_hdr = df_hdr_raw.copy()\n",
    "    for i2, row in df_hdr_raw.iterrows():\n",
    "        df_hdr.loc[i2] = row.str.split(r'\\t').str[1]\n",
    "    \n",
    "    # Fix dates\n",
    "    df_dat.index = pd.DatetimeIndex(df_dat.index)\n",
    "    \n",
    "    # Drop rows with bad times (NAT)\n",
    "    df_dat = df_dat[pd.notnull(df_dat.index)]\n",
    "    \n",
    "    var_name_full  = Var_names[i]\n",
    "    var_units = Var_units[i]\n",
    "    # Set column names as station ID\n",
    "    df_dat.columns = df_hdr.loc['Station Number'].values\n",
    "#     print(df_hdr.loc['Station Number'].values)\n",
    "    # Set time\n",
    "    df_dat.index.names = ['Time_MST'] # Kabir was unsure, check this time zone\n",
    "    \n",
    "    # Store as dict\n",
    "    ds_dict[var_name_full]   = df_dat\n",
    "    unit_dict[var_name_full] = var_units\n",
    "    hrd_dict[var_name_full]  = df_hdr\n",
    "    \n",
    "# Memory clean up\n",
    "df_all = None\n",
    "df_hdr_raw = None\n",
    "df_dat = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Constains duplicate time values...  So need to manualy create datasets, then fix time, then merge\n",
    "da_fill_list = []\n",
    "for k in ds_dict.keys():\n",
    "    print(k)\n",
    "    ds_temp = xr.DataArray(ds_dict[k], coords = {'Time_MST':ds_dict[k].index, 'staID':ds_dict[k].columns}, dims = ('Time_MST','staID'))\n",
    "    ds_temp.name = k\n",
    "    Time_new = np.arange(ds_temp.Time_MST.values[0], ds_temp.Time_MST.values[-1], dtype='datetime64[h]')\n",
    "    ds_fill = ds_temp.reindex({'Time_MST':Time_new})\n",
    "    da_fill_list.append(ds_fill)\n",
    "ds_dict = None # Memory clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Merge into netcdf\n",
    "ds = xr.merge(da_fill_list)\n",
    "da_fill_list = None # Memory clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Convert units\n",
    "ds['WindSpeed'] = ds['WindSpeed'] * 1000/(60*60) # km/h to m/s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## ADD UNITS\n",
    "# Add variable attributes (units), and fix variable names (remove spaces)\n",
    "for cvar in ds.data_vars:\n",
    "    # add units as attributes\n",
    "    ds.get(cvar).attrs['unit'] = unit_dict[cvar]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Grab metadata\n",
    "metadata = hrd_dict['AirTemperature'] # Might need to merge all\n",
    "metadata.columns = metadata.loc['Station Number'].values\n",
    "metadata = metadata.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Add station metadata\n",
    "ds['station_name'] = xr.DataArray(metadata['Station Name'], coords={'staID':metadata.index}, dims='staID')\n",
    "ds['Lat'] = xr.DataArray(metadata['Latitude'].astype(float),coords={'staID':metadata.index}, dims='staID')\n",
    "ds['Lon'] = xr.DataArray(metadata['Longitude'].astype(float),coords={'staID':metadata.index}, dims='staID')\n",
    "# Orig filese are missing Elevation, so used a separate metadata file\n",
    "# Import metadata for each station\n",
    "metadata_EL = pd.read_csv(meta_file_path,index_col='staID',delimiter=',',na_values=[0])\n",
    "metadata_EL = metadata_EL.loc[ds.staID.values]\n",
    "ds['Elevation'] = xr.DataArray(metadata_EL['Elevation'],coords={'staID':metadata_EL.index}, dims='staID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Move to coords\n",
    "ds.set_coords(['station_name','Elevation','Lat','Lon'], inplace=True)\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Find stations missing Elevation data (or is zero (wrong))\n",
    "X = ds.Elevation.where(ds.Elevation.isnull(), drop=True)\n",
    "list(zip(X.station_name.values,X.staID.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Reindex to have continous time steps\n",
    "# Time_UTC_new = np.arange(ds.Time_UTC.values[0], ds.Time_UTC.values[-1], dtype='datetime64[h]')\n",
    "# ds_fill = ds.reindex({'Time_UTC':Time_UTC_new})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Add Network(s)\n",
    "ds.coords['network'] = xr.DataArray([network for x in ds.staID], dims='staID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # Write out to netcdf by year\n",
    "# os.chdir(netcdf_dir)\n",
    "# years, datasets = zip(*ds.groupby('Time_MST.year'))\n",
    "# paths = ['%s.nc' % y for y in years]\n",
    "# xr.save_mfdataset(datasets, paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save as netcdf file\n",
    "ds.to_netcdf(netcdf_file_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
